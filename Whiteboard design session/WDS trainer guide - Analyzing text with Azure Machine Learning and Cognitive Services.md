![Microsoft Cloud Workshops](https://github.com/Microsoft/MCW-Template-Cloud-Workshop/raw/main/Media/ms-cloud-workshop.png "Microsoft Cloud Workshops")

<div class="MCWHeader1">
Analyzing text with Azure Machine Learning and Cognitive Services
</div>

<div class="MCWHeader2">
Whiteboard design session trainer guide
</div>

<div class="MCWHeader3">
November 2021
</div>

Information in this document, including URL and other Internet Web site references, is subject to change without notice. Unless otherwise noted, the example companies, organizations, products, domain names, e-mail addresses, logos, people, places, and events depicted herein are fictitious, and no association with any real company, organization, product, domain name, e-mail address, logo, person, place or event is intended or should be inferred. Complying with all applicable copyright laws is the responsibility of the user. Without limiting the rights under copyright, no part of this document may be reproduced, stored in or introduced into a retrieval system, or transmitted in any form or by any means (electronic, mechanical, photocopying, recording, or otherwise), or for any purpose, without the express written permission of Microsoft Corporation.

Microsoft may have patents, patent applications, trademarks, copyrights, or other intellectual property rights covering subject matter in this document. Except as expressly provided in any written license agreement from Microsoft, the furnishing of this document does not give you any license to these patents, trademarks, copyrights, or other intellectual property.

The names of manufacturers, products, or URLs are provided for informational purposes only, and Microsoft makes no representations and warranties, either expressed, implied, or statutory, regarding these manufacturers or the use of the products with any Microsoft technologies. The inclusion of a manufacturer or product does not imply endorsement of Microsoft of the manufacturer or product. Links may be provided to third-party sites. Such sites are not under the control of Microsoft and Microsoft is not responsible for the contents of any linked site or any link contained in a linked site, or any changes or updates to such sites. Microsoft is not responsible for webcasting or any other form of transmission received from any linked site. Microsoft is providing these links to you only as a convenience, and the inclusion of any link does not imply endorsement of Microsoft of the site or the products contained therein.

© 2021 Microsoft Corporation. All rights reserved.

Microsoft and the trademarks listed at <https://www.microsoft.com/en-us/legal/intellectualproperty/Trademarks/Usage/General.aspx> are trademarks of the Microsoft group of companies. All other trademarks are the property of their respective owners.

**Contents**

<!-- TOC -->

- [Trainer information](#trainer-information)
  - [Role of the trainer](#role-of-the-trainer)
  - [Whiteboard design session flow](#whiteboard-design-session-flow)
  - [Before the whiteboard design session: How to prepare](#before-the-whiteboard-design-session-how-to-prepare)
  - [During the whiteboard design session: Tips for an effective whiteboard design session](#during-the-whiteboard-design-session-tips-for-an-effective-whiteboard-design-session)
- [Analyzing text with Azure Machine Learning and Cognitive Services whiteboard design session student guide](#analyzing-text-with-azure-machine-learning-and-cognitive-services-whiteboard-design-session-student-guide)
  - [Abstract and learning objectives](#abstract-and-learning-objectives)
  - [Step 1: Review the customer case study](#step-1-review-the-customer-case-study)
    - [Customer situation](#customer-situation)
    - [Customer needs](#customer-needs)
    - [Customer objections](#customer-objections)
    - [Infographic for common scenarios](#infographic-for-common-scenarios)
  - [Step 2: Design a proof of concept solution](#step-2-design-a-proof-of-concept-solution)
  - [Step 3: Present the solution](#step-3-present-the-solution)
  - [Wrap-up](#wrap-up)
  - [Additional references](#additional-references)
- [Analyzing text with Azure Machine Learning and Cognitive Services whiteboard design session trainer guide](#analyzing-text-with-azure-machine-learning-and-cognitive-services-whiteboard-design-session-trainer-guide)
  - [Step 1: Review the customer case study](#step-1-review-the-customer-case-study-1)
  - [Step 2: Design a proof of concept solution](#step-2-design-a-proof-of-concept-solution-1)
  - [Step 3: Present the solution](#step-3-present-the-solution-1)
  - [Wrap-up](#wrap-up-1)
  - [Preferred target audience](#preferred-target-audience)
  - [Preferred solution](#preferred-solution)
  - [Checklist of preferred objection handling](#checklist-of-preferred-objection-handling)
  - [Customer quote (to be read back to the attendees at the end)](#customer-quote-to-be-read-back-to-the-attendees-at-the-end)

<!-- /TOC -->

# Trainer information

Thank you for taking time to support the whiteboard design sessions as a trainer!

## Role of the trainer

An amazing trainer:

- Creates a safe environment in which learning can take place.

- Stimulates the participant's thinking.

- Involves the participant in the learning process.

- Manages the learning process (on time, on topic, and adjusting to benefit participants).

- Ensures individual participant accountability.

- Ties it all together for the participant.

- Provides insight and experience to the learning process.

- Effectively leads the whiteboard design session discussion.

- Monitors quality and appropriateness of participant deliverables.

- Effectively leads the feedback process.

## Whiteboard design session flow

Each whiteboard design session uses the following flow:

**Step 1: Review the customer case study (15 minutes)**

**Outcome**

Analyze your customer's needs.

- Customer's background, situation, needs and technical requirements

- Current customer infrastructure and architecture

- Potential issues, objectives and blockers

**Step 2: Design a proof of concept solution (60 minutes)**

**Outcome**

Design a solution and prepare to present the solution to the target customer audience in a 15-minute chalk-talk format.

- Determine your target customer audience.

- Determine customer's business needs to address your solution.

- Design and diagram your solution.

- Prepare to present your solution.

**Step 3: Present the solution (30 minutes)**

**Outcome**

Present solution to your customer:

- Present solution

- Respond to customer objections

- Receive feedback

**Wrap-up (15 minutes)**

- Review preferred solution

## Before the whiteboard design session: How to prepare

Before conducting your first whiteboard design session:

- Read the Student guide (including the case study) and Trainer guide.

- Become familiar with all key points and activities.

- Plan the point you want to stress, which questions you want to drive, transitions, and be ready to answer questions.

- Prior to the whiteboard design session, discuss the case study to pick up more ideas.

- Make notes for later.

## During the whiteboard design session: Tips for an effective whiteboard design session

**Refer to the Trainer guide** to stay on track and observe the timings.

**Do not expect to memorize every detail** of the whiteboard design session.

When participants are doing activities, you can **look ahead to refresh your memory**.

- **Adjust activity and whiteboard design session pace** as needed to allow time for presenting, feedback, and sharing.

- **Add examples, points, and stories** from your own experience. Think about stories you can share that help you make your points clearly and effectively.

- **Consider creating a "parking lot"** to record issues or questions raised that are outside the scope of the whiteboard design session or can be answered later. Decide how you will address these issues, so you can acknowledge them without being derailed by them.

***Have fun**! Encourage participants to have fun and share!*

**Involve your participants.** Talk and share your knowledge but always involve your participants, even while you are the one speaking.

**Ask questions** and get them to share to fully involve your group in the learning process.

**Ask first**, whenever possible. Before launching into a topic, learn your audience's opinions about it and experiences with it. Asking first enables you to assess their level of knowledge and experience, and leaves them more open to what you are presenting.

**Wait for responses**. If you ask a question such as, "What's your experience with (fill in the blank)?" then wait. Do not be afraid of a little silence. If you leap into the silence, your participants will feel you are not serious about involving them and will become passive. Give participants a chance to think, and if no one answers, patiently ask again. You will usually get a response.

# Analyzing text with Azure Machine Learning and Cognitive Services whiteboard design session student guide

## Abstract and learning objectives

In this whiteboard design session, you work with a group to design a solution that combines both pre-built artificial intelligence (AI) in the form of Text Analytics API from Cognitive Services with custom AI in the form of services built and deployed with Azure Machine Learning services. You will learn to create intelligent solutions atop unstructured text data by designing and implementing a text analytics pipeline. You will discover how to build a binary classifier that can be used to classify the textual data. You will also learn how to deploy multiple kinds of predictive services using Azure Machine Learning and learn to integrate with the Text Analytics API from Cognitive Services.

At the end of this whiteboard design session, you will be better able to design solutions leveraging Azure Machine Learning services and Cognitive Services.

## Step 1: Review the customer case study 

**Outcome**

Analyze your customer's needs.

Timeframe: 15 minutes

Directions: With all participants in the session, the facilitator/SME presents an overview of the customer case study along with technical tips.

1. Meet your team members and trainer.

2. Read all directions for steps 1-3 in the student guide.

3. As a team, review the following customer case study.

### Customer situation

Contoso Ltd is a large corporation headquartered in the United States that provides insurance packages for U.S. consumers. Its products include accident and health insurance, life insurance, travel, home, and auto coverage.

Contoso is looking to build a next-generation platform for its insurance products and has identified claims processing as the first area in which they would like to focus its efforts. Currently, customers submit a claim using either the website, their mobile app, or by speaking with a live agent.

A claim includes the following information:

- Information about the insured (contact information, policy number, etc.)

- Free text responses describing the claim (details of what happened, what was affected, the conditions in which the incident occurred)

When processing a claim, agents face multiple challenges that add significantly to Contoso's costs. These challenges include the time it takes for an agent to read through and process the content submitted with each claim. While each claim is stored in a database, the details about the claim, including the free-text responses are stored as opaque attachments. Agents typically have to pull up the claim by the claim number or the insured's contact information and manually read through the attachments.

Also, there are some common challenges that Contoso is hoping they could automate away. According to Francine Fischer, CIO, there are two sets of issues where they envision amplifying their agents' capabilities with AI.

One set of such issues deals with the free-text responses. The first issue Contoso identified is that each claim detail should be automatically classified as either home or auto based on the text. This classification should be displayed within the claim summary, so agents can quickly assess whether they are dealing with a home claim, an auto claim, or a claim with a mixture of the two.

The second issue is Contoso would like to experiment with applying text analysis to the claim text. They know most customers are either factual in their description (a neutral sentiment) or slightly unhappy (a more negative sentiment). They believe that negative sentiment can be an indicator to claim text that involves a more severe situation, which might warrant an agent's expedited review. Furthermore, they would like to understand any positive or negative opinions the customers have expressed in their responses, and quickly identify key concepts in the claims text. They would also like to detect the language of the claims and identify any personal information included by customers in their responses.

The third issue with the free text is that some of the responses are long. When agents are shifting between claims, it can be difficult for them to recall which response had the details they need. Contoso would like to experiment with an automatic summarization of long claims that produces a summary of about 30 words in length. This summarization would enable the agent to get the gist before reading the full claim and quickly remind themselves of the claim when revisiting it.

As a final step, they would like to organize the information generated from text classification, text analysis and text summarization that can be then fed into their Agent portal, an application where they can more quickly review and act on claim information.

As a first step towards their bigger goals, Contoso would like to build a proof of concept (PoC) for an intelligent solution that could automate all the above. They would like to develop this PoC to build upon the claims submission solution they already have running in Azure. The existing solution consists of a Web App for claims submission and a SQL Database for claim storage. Contoso Ltd. believes this might be possible using AI, machine learning, or deep learning and would like to build a proof of concept to understand how far they can go using these technologies.

### Customer needs

1. We receive a lot of useful information in the free-text responses. However, because the free-text responses can be lengthy, agents sometimes skip over them and miss vital details or spend too much time looking for a particular point when returning to a claim. We aren't confident we can automate this step. Still, we would like to have a standardized process that identifies the key units of actionable information in a claim and pulls these units of information out into a separate section that agents can more easily review and then be able to view and read both the summary and the entire text of the claims.

2. We are looking to amplify our agents' capabilities and improve their claims processing capabilities - not replace them. We want a solution that does the same.

### Customer objections

1. We are skeptical about all the hype surrounding these "AI" solutions. It's hard to know what is feasible versus what is not possible with today's technology and Azure.

2. We know that there are pre-built AI, Automated ML, and custom AI options available. We are confused as to when to choose one over the other.

3. We expect some part of our solution would require deep learning. Do you have any prescriptive guidance on how we might choose between investing in understanding and using TensorFlow or the Microsoft Cognitive Toolkit (CNTK)?

### Infographic for common scenarios

![In the Training a classification model with text diagram, Document labels points to Supervised ML or DL Algorithm, which points to Classification Model. Documents points to Text Normalization, which points to Feature Extraction, which points to Supervised ML or DL Algorithm. Vectors points to a table of words and percentages.](media/image2.png "Training a classification model with text diagram")

![The Predicting a classification from text diagram has Documents, which points to Text Normalization, which points to Feature Extraction, which points to Classification Model, which points to Document Labels. Vectors points to a table of words and percentages.](media/image3.png "Predicting a classification from text diagram")

## Step 2: Design a proof of concept solution

**Outcome**

Design a solution and prepare to present the solution to the target customer audience in a 15-minute chalk-talk format.

Timeframe: 60 minutes

**Business needs**

Directions: With your team, answer the following questions and be prepared to present your solution to others:

1. Who will you present this solution to? Who is your target customer audience? Who are the decision makers?

2. What customer business needs do you need to address with your solution?

**Design**

Directions: With your team, respond to the following questions:

_High-level architecture_

1. Without getting into the details (the following sections will address the details), diagram your initial vision for handling the top-level requirements for processing the claims textual data, photos, and enabling search. You will refine this diagram as you proceed.

_Classifying claim-text data_

1. What is the general pipeline for approaching the training of text analytic models such as this? What are the general steps you need to take to prepare the text data for performing tasks like classification?

2. What data would they need to train the model?

3. Contoso wants to understand some of the common approaches to handle texts for machine learning. Is there a recommended approach to dealing with long descriptive texts that are typically found in claims data?

4. Contoso understands they should use a classification algorithm for this problem. They have asked if a Deep Neural Network could be trained against the text to recognize home or auto classifications. Could they use a DNN for this?

5. For this scenario, Contoso has indicated an interest in using TensorFlow but is concerned about the complexity of jumping right in. They wonder if Keras would provide an easier framework they could use as a steppingstone to the full-blown TensorFlow, which would enable them to build TensorFlow compatible models so that they can "graduate" to using TensorFlow when the team is ready?

6. What would a Long Short-Term Memory (LSTM) recurrent neural network that performs this classification look like? Show a snippet of a single layer of an unrolled LSTM network and the binary classification output at the network's last step.

7. Assuming they will be using an LSTM recurrent neural network to train the classifier using Keras, pseudo code the code you would write to construct the network you just illustrated.

8. Next, pseudo code how they would define the optimizer, loss function and fit the model to the vectorized data and the labels.

9. With the trained model in hand, pseudo code how the model would predict the class of a given claim text. What would the output of the prediction be? How would you interpret the value?

10. Describe how you would deploy this trained model at a high level to be available as a web service integrated with the rest of the solution.

_Automated machine learning_

1. Can Contoso apply automated machine learning for text classification?

2. Can they really expect to create performant models using automated machine learning?

_Free-text Analytics_

1. How would you recommend Contoso identify the sentiment, opinions, and key phrases in the free-response text provided associated with a claim? Would this require you to build a custom AI model? Is there a pre-built AI service you could use?

2. For the solution you propose, what is the range of value of the sentiment score, and how would you interpret that value?

3. Write some pseudo code to describe how the Text Analytics APIs could be used for Contoso's text analytics use cases. (The actual solution may use the Python SDK or REST APIs)

_Summarizing claim text_

1. The team at Contoso has heard about a Python library called Gensim, which has a summarize function. Given an input of text, it can extract a summary of the desired length. Contoso would like their PoC to implement its summarization functionality initially using Gensim. However, the process they follow to deploy the summarization capability should also enable them to replace Gensim with another library or with the use of their own custom trained models if desired down the road. Describe how Contoso should deploy the summarization service to meet these requirements?

2. Discuss with Contoso team the Text Analytics extractive summarization capability that is in preview (as of November 2021) and how that can be used in place of Gensim when it becomes generally available.

**Prepare**

Directions: As a team:

1. Identify any customer needs that are not addressed with the proposed solution.

2. Identify the benefits of your solution.

3. Determine how you will respond to the customer's objections.

Prepare a 15-minute chalk-talk style presentation to the customer.

## Step 3: Present the solution

**Outcome**

Present a solution to the target customer audience in a 15-minute chalk-talk format.

Timeframe: 30 minutes

**Presentation**

Directions:

1. Pair with another team.

2. One group is the Microsoft team and the other is the customer.

3. The Microsoft team presents their proposed solution to the customer.

4. The customer makes one of the objections from the list of objections.

5. The Microsoft team responds to the objection.

6. The customer team gives feedback to the Microsoft team.

7. Switch roles and repeat Steps 2-6.

## Wrap-up

Timeframe: 15 minutes

Directions: Reconvene with the larger group to hear the facilitator/SME share the preferred solution for the case study.

## Additional references

|                                                       |                                                                                                   |
| ----------------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| **Description**                                       | **Links**                                                                                         |
| Azure Machine Learning service                        | <https://docs.microsoft.com/en-us/azure/machine-learning/service/overview-what-is-azure-ml>       |  |
| Deploying Web Services                                | <https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where> |
| Overview of Keras                                   | <https://keras.io/> |
| Overview of TensorFlow                                | <https://www.tensorflow.org/> |
| Term Frequency-Inverse Document Frequency (TF-IDF) vectorization | <https://en.wikipedia.org/wiki/Tf-idf> |
| GloVe: Global Vectors for Word Representation | <https://nlp.stanford.edu/projects/glove/>  |
| Research Paper: "GloVe: Global Vectors for Word Representation" | <https://nlp.stanford.edu/pubs/glove.pdf>  |
| Word2vec word embeddings | <https://en.wikipedia.org/wiki/Word2vec>  |
| Overview of the Text Analytics API Cognitive Service  | <https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/overview>               |
| Text Analytics Extractive Summarization | <https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/extractive-summarization>               |

# Analyzing text with Azure Machine Learning and Cognitive Services whiteboard design session trainer guide

## Step 1: Review the customer case study

- Check in with your participants to introduce yourself as the trainer.

- Ask, "What questions do you have about the customer case study?"

- Briefly review the steps and timeframes of the whiteboard design session.

- Ready, set, go! Let participants begin.

## Step 2: Design a proof of concept solution

- Check in with your teams to ensure that they are transitioning from step to step on time.

- Provide feedback on their responses to the business needs and design.

  - Try asking questions first that will lead the participants to discover the answers on their own.

- Provide feedback for their responses to the customer's objections.

  - Try asking questions first that will lead the participants to discover the answers on their own.

## Step 3: Present the solution

- Determine which groups will be paired together before Step 3 begins.

- For the first round, assign one group as the presenting team and the other as the customer.

- Have the presenting team present their solution to the customer team.

  - Have the customer team provide one objection for the presenting team to respond to.

  - The presentation, objections, and feedback should take no longer than 15 minutes.

  - If needed, the trainer may also provide feedback.

## Wrap-up

- Have participants reconvene with the larger session group to hear the facilitator/SME share the following preferred solution.

## Preferred target audience

Francine Fischer, CIO of Contoso Ltd.

The primary audience is the business and technology decision-makers. From the case study scenario, this would include the Director of Analytics. Usually, we talk to the infrastructure managers, who report to the chief information officers (CIOs), or application sponsors, like a vice president (VP) line of business (LOB), or chief marketing officer (CMO), or to those that represent the business unit IT or developers that report to application sponsors.

## Preferred solution

_High-level architecture_

1. Without getting into the details (the following sections will address the details), diagram your initial vision for handling the top-level requirements for processing the claims textual data. You will refine this diagram as you proceed.

    After speaking with its team at Microsoft, Contoso decided to design their PoC solution in Azure. They would continue to use the web app and SQL database they already have running in Azure to handle claim submissions. They could build a claim enrichment pipeline by invoking a sequence of Text Analytics APIs and custom AI in the form of services built and deployed with Azure Machine Learning services.

     ![The High-level architectural solution begins with a Claim, which points to Claims submission WebApp. The WebApp then points to Text Analytics, and Containerized Services, which includes a Classification Service and a Summary Service that both processes claim text.](media/new_arch.png "High-level architectural solution")

   The claim processing pipeline would invoke a mixture of pre-built AI, in the form of Cognitive Services Text Analytics APIs and custom AI in the form of Azure ML services, to process the claim text. The custom models used for processing claims' text would be trained in Azure Machine Learning compute clusters. These models could also be directly deployed to containerized services such as Azure Container Instance (ACI) or Azure Kubernetes Service (AKS) using the Azure Machine Learning Service Python SDK. The Text Analytics APIs could be invoked to provide key analytics such as, sentiment analysis, opinion mining, key phrase extraction, and language and PII detection. The enriched claims data is then saved in the SQL database to serve the Agent Web portal.

_Classifying claim text data_

1. What is the general pipeline for approaching the training of text analytic models such as this? What are the general steps you need to take to prepare the text data for performing tasks like classification?

    ![In the high-level steps for training a classification model with text diagram, Document labels points to Supervised ML or DL Algorithm, which points to Classification Model. Documents points to Text Normalization, which points to Feature Extraction, which points to Supervised ML or DL Algorithm.](media/image6.png "High-level steps for training a classification model with the text")

    As the above diagram illustrates, the general pipeline begins by pre-processing or normalizing the text. This step typically includes breaking the text into sentences and word tokens, standardizing the spelling of words, and removing overly common words (called stop words). This phase's output is typically a multi-dimensional array consisting of an array of documents, each having an array of sentences, with each sentence having an array of words. The next step is feature extraction, which creates a numeric representation of the textual documents. A "vocabulary" of unique words is identified during feature extraction, and each word becomes a column in the output. Each row represents a document. The value in each cell is typically a measure of the relative importance of that word in the document. If a word from the vocabulary does not appear, then that cell has a zero value in that column. This approach enables machine learning algorithms, which operate against arrays of numbers, to also run against text. Deep learning algorithms operate on tensors, which are also vectors (or arrays of numbers), so this approach is also valid for preparing text for use with a deep learning algorithm.

2. What data would they need to train the model?

    Contoso would need to have a certain amount of historical claim text and have it labeled as home or auto to train a model.

3. Contoso wants to understand some of the common approaches to handle texts for machine learning. Is there a recommended approach to dealing with long descriptive texts that are typically found in claims data?

    Machine learning models require numeric data as inputs. Thus, when you are working with text, you convert words or sentences into numeric vector representation as part of feature extraction. There are several approaches to vectorize textual data, that include strategies like [Term Frequency-Inverse Document Frequency (TF-IDF) vectorization](https://en.wikipedia.org/wiki/Tf-idf), or use of word embedding like [Word2vec](https://en.wikipedia.org/wiki/Word2vec) or [Global Vectors (GloVe)](https://nlp.stanford.edu/pubs/glove.pdf).

    TF-IDF's approach gives less importance to words common in most documents and assigns higher importance to words that appear more frequently in fewer documents. Thus TF-IDF assigns weights to words that signify their relevance in the documents. There are some disadvantages to the TF-ID approach. Most notably, it makes no use of semantic similarities between words.

    The use of embedding to represent words or sentences is considered the-state-of-the art in the natural language processing (NLP) field. The most commonly used word embedding with Deep Neural Network (DNN) is either Word2vec or GloVe. Both Word2vec and GloVe are known to perform similarly, with GloVe claiming to outperform its peers on similarity tasks and named entity recognition.

    In the scenario, given the claims data's descriptive nature, they recommend using pre-trained GloVe word embedding from [nlp.stanford.edu](https://nlp.stanford.edu/projects/glove/) for vector representation of words.

4. Contoso understands they should use a classification algorithm for this problem. They have asked if a Deep Neural Network (DNN) could be trained against the text to recognize home or auto classifications. Could they use a DNN for this?

    Yes, they could use a type of DNN called the Long Short-Term Memory (LSTM) recurrent neural network that is shown to work well for text classification problems, especially when used in conjunction with word embedding such as GloVe for word vectorization.

5. For this scenario, Contoso has indicated an interest in using TensorFlow but is concerned about the complexity of jumping right in. They wonder if Keras would provide an easier framework they could use as a steppingstone to the full-blown TensorFlow, which would enable them to build TensorFlow compatible models so that they can "graduate" to using TensorFlow when the team is ready?

    TensorFlow is a robust framework for performing machine learning, including building neural networks. The Keras library builds upon TensorFlow and provides an easy-to-use and understand high-level API for implementing deep neural networks, complete with tutorials and examples. Models constructed with Keras are TensorFlow models, so if they choose to move fully towards the lower-level TensorFlow API's, they could do so without re-creating the models.

6. What would a Long Short-Term Memory (LSTM) recurrent neural network that performs this classification look like? Show a snippet of a single layer of an unrolled LSTM network and the binary classification output at the network's last step.

    ![The figure shows a snippet of a single layer of an unrolled LSTM network, and the binary classification output layer at the last step of the network.](media/lstm.png "Unrolled LSTM network")

7. Assuming they will be using an LSTM recurrent neural network to train the classifier using Keras, pseudo code the code you would write to construct the network you just illustrated.

    ```python
    model = Sequential()

    model.add(embedding_layer)

    model.add(LSTM(100, ..., ...))

    model.add(Dense(2))

    model.add(Activation('sigmoid'))
    ```

8. Next, pseudo code how they would define the optimizer, loss function and fit the model to the vectorized data and the labels.

    ```python
    opt = keras.optimizers.Adam(lr = ...)

    model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['accuracy'])

    model.fit(X_train, y_train, epochs = ..., batch_size = ..., validation_data = ...)
    ```

9. With the trained model in hand, pseudo code how the model would predict the class of a given claim text. What would the output of the prediction be? How would you interpret the value?

    ```python
    test_claim = ['I crashed my car into a pole.']

    pred = model.predict(test_claim)
    ```

    The output of pred is an array of the confidence that label is a 0 or a 1. For example:

    ```python
    array([ [0.22, 0.78] ])
    ```

    Could be interpreted to indicate that a prediction of 1 ("auto insurance claim") with a confidence of 78%.

10. Describe how you would deploy this trained model at a high level to be available as a web service integrated with the rest of the solution.

     The trained model is saved to a file. This file is then loaded by web service code that re-creates the model architecture and loads the model weights. The web service code can then run classifications using the model. You could deploy this service using Azure Machine Learning service, which would capture the web service in a container, and then deploy it to Azure Container Service where any REST client can invoke it.

_Automated machine learning_

1. Can Contoso apply automated machine learning for text classification?

    Azure automated machine learning picks an algorithm and hyperparameters for you and generates a model ready for deployment. Azure automated machine learning can be applied for text classification problems such as automatically classifying the claims text as either home or auto.

2. Can they really expect to create performant models using automated machine learning?

    Automated machine learning in Azure Machine Learning helps to simplify and expedite the process of producing a performant model. You can create and run automated machine learning experiments in code using the [Azure ML Python SDK](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train) or if you prefer a no code experience, you can also create your automated machine learning experiments in the [Azure portal](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-create-portal-experiments). It’s a useful tool in the toolkit that can be leveraged by both non-data scientists as well as professional data scientists.

_Free-text Analytics_

1. How would you recommend Contoso identify the sentiment, opinions, and key phrases in the free-response text provided associated with a claim? Would this require you to build a custom AI model? Is there a pre-built AI service you could use?

    Contoso should use the Text Analytics API from Cognitive Services for scoring the sentiment, extracting opinion, and identifying key phrases of the claim text. By doing so, they would not have to build or train a custom model, nor have the requirement of having the data to do so.

2. For the solution you propose, what is the range of value of the sentiment score, and how would you interpret that value?

    The Text Analytics API returns confidence scores from 0 to 1 for positive, neutral, and negative sentiments. Higher confidence score for a given sentiment implies higher probability of that sentiment being present in the text.

3. Write some pseudo code to describe how the Text Analytics APIs could be used for Contoso's text analytics use cases. (The actual solution may use the Python SDK or REST APIs)

    The steps required to use the Text Analytics APIs is as follows:
    - Import the dependent libraries
    - Create a "client" in the code to interact with the web service
    - Read in the claim(s)
    - Send the claim text to the Text Analytics API to retrieve the information specifically needed (i.e., sentiments, extract key phrases, etc.)
    - Retrieve the results
    - Parse the returned values and send them to the next step of the solution chain

   ```python
    from azure.core.credentials import AzureKeyCredential
    from azure.ai.textanalytics import TextAnalyticsClient
    
    # Create the Text Analytics Client
    credential = AzureKeyCredential(key)
    client = TextAnalyticsClient(endpoint=endpoint, credential=credential)

    # Analyze sentiments in the claims text
    claim = "..."
    response = client.analyze_sentiment(documents=[claim])[0]

    # Retrieve the sentiment scores from the response
    overall_positive_score = response.confidence_scores.positive
    overall_neutral_score = response.confidence_scores.neutral
    overall_negative_score = response.confidence_scores.negative
    ```

_Summarizing claim text_

1. The team at Contoso has heard about a Python library called Gensim, which has a summarize function. Given an input of text, it can extract a summary of the desired length. Contoso would like their PoC to implement its summarization functionality initially using Gensim. However, the process they follow to deploy the summarization capability should also enable them to replace Gensim with another library or use their own custom trained models if desired down the road. Describe how Contoso should deploy the summarization service to meet these requirements?

    Azure Machine Learning service can be used to deploy web services that do not have a model. While the API used to perform the deployment requires a model argument, the argument can refer to any file, and it does not require the use of the file during the web service runtime. Therefore, Contoso could deploy a web service that uses Gensim to perform summarization.

2. Discuss with Contoso team the Text Analytics extractive summarization capability that is in preview (as of November 2021) and how that can be used in place of Gensim when it becomes generally available.

    The extractive summarization is a feature in Azure Text Analytics produces a summary by extracting sentences that collectively represent the most important or relevant information within the original content. This feature is designed to shorten content that users consider too long to read. The AI models used by the API are provided by the service, you just have to send content for analysis. Text Analytics extractive summarization is a preview capability (as of Nov 2021) and should not be deployed in any production use. However, once it becomes generally available, it will be part of the Text Analytics APIs and the summarization APIs can be then leveraged in a similar fashion as the other Text Analytics APIs.

## Checklist of preferred objection handling

1. We are skeptical about all the hype surrounding these "AI" solutions. It's hard to know what is feasible versus what is not possible with today's technology and Azure.

    While it is true that there is a lot of hype around AI, the ability to deploy solutions that use data, machine learning, and deep learning to create an application with "AI" capabilities is real and is possible in Azure. Azure provides a wide range of services to address the needs of AI, from pre-built AI capabilities in Cognitive Services, to Automated Machine Learning in Azure Machine Learning, to services that help you to build, train, and deploy your custom AI capabilities using the Azure Machine Learning service and other services from the Microsoft AI stack.

2. We know that there are both pre-built AI and custom AI options available. We are confused as to when to choose one over the other.

    When proving the value of an A.I. or machine learning solution, it can be helpful to start with ready-to-use solutions and then progress into more customized solutions. So, in the case of Azure A.I. services, start with cognitive services like, the Text Analytics API. If more control is needed, try Auto ML. For a fully customizable solution, consider building your own model in Azure ML.

3. We expect some part of our solution would require deep learning. Do you have any prescriptive guidance on how we might choose between investing in understanding and using TensorFlow or the Microsoft Cognitive Toolkit (CNTK)?

    Both TensorFlow and the Microsoft Cognitive Toolkit solve similar problems and have been used successfully by many companies for deep learning solutions. At present, it appears that TensorFlow has a much larger community base and interest level, which can be measured simply by the number of stars it has in its GitHub project (which is an order of magnitude larger than that of the Microsoft Cognitive Toolkit). The community's size means that it is likely you will more easily find help online for issues with TensorFlow versus the Microsoft Cognitive Toolkit, which is why it may be a good reason to start with TensorFlow.

## Customer quote (to be read back to the attendees at the end)

"We are excited by the possibilities made real when we use AI to amplify the capabilities of our agents."

Francine Fischer, CIO of Contoso Ltd.
